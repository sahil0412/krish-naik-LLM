{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description: Next Word Prediction Using LSTM\n",
    "#### Project Overview:\n",
    "\n",
    "This project aims to develop a deep learning model for predicting the next word in a given sequence of words. The model is built using Long Short-Term Memory (LSTM) networks, which are well-suited for sequence prediction tasks. The project includes the following steps:\n",
    "\n",
    "1- Data Collection: We use the text of Shakespeare's \"Hamlet\" as our dataset. This rich, complex text provides a good challenge for our model.\n",
    "\n",
    "2- Data Preprocessing: The text data is tokenized, converted into sequences, and padded to ensure uniform input lengths. The sequences are then split into training and testing sets.\n",
    "\n",
    "3- Model Building: An LSTM model is constructed with an embedding layer, two LSTM layers, and a dense output layer with a softmax activation function to predict the probability of the next word.\n",
    "\n",
    "4- Model Training: The model is trained using the prepared sequences, with early stopping implemented to prevent overfitting. Early stopping monitors the validation loss and stops training when the loss stops improving.\n",
    "\n",
    "5- Model Evaluation: The model is evaluated using a set of example sentences to test its ability to predict the next word accurately.\n",
    "\n",
    "6- Deployment: A Streamlit web application is developed to allow users to input a sequence of words and get the predicted next word in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Data Collection\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import  pandas as pd\n",
    "\n",
    "## load the dataset\n",
    "data=gutenberg.raw('shakespeare-hamlet.txt')\n",
    "## save to a file\n",
    "with open('hamlet.txt','w') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##laod the dataset\n",
    "with open('hamlet.txt','r') as file:\n",
    "    text=file.read().lower()\n",
    "\n",
    "## Tokenize the text->(creating indexes for words)\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words=len(tokenizer.word_index)+1\n",
    "total_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create input sequences\n",
    "# Suppose sentence = \"hello world\", and the tokenizer has a word index like {'hello': 1, 'world': 2}.\n",
    "# tokenizer.texts_to_sequences([\"hello world\"]) would output [[1, 2]].\n",
    "# The [0] at the end extracts the first list, resulting in [1, 2].\n",
    "\n",
    "input_sequences=[]\n",
    "for line in text.split('\\n'):\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence=token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pad Sequences\n",
    "max_sequence_len=max([len(x) for x in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4],\n",
       "       [   0,    0,    0, ..., 1047,    4,  193]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create predicitors and label\n",
    "import tensorflow as tf\n",
    "x,y=input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,    1],\n",
       "       [   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 687,    4,   45, ..., 1047,    4,  193])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=tf.keras.utils.to_categorical(y,num_classes=total_words)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 13, 150)           150600    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13, 150)           0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219418 (4.65 MB)\n",
      "Trainable params: 1219418 (4.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Train our LSTM RNN\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,GRU\n",
    "\n",
    "## Define the model\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "\n",
    "# When return_sequences=True, the LSTM layer returns the full sequence of outputs from all time steps, \n",
    "# not just the output of the last time step. This means that the output will be a 3D array with dimensions \n",
    "# (batch_size, time_steps, units).\n",
    "\n",
    "# This is useful when stacking multiple LSTM layers or \n",
    "# when you need each time step’s output for further processing, \n",
    "# such as for sequence-to-sequence models or attention mechanisms.\n",
    "\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "# #Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRU RNN\n",
    "## Define the model\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "model.add(GRU(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(100))\n",
    "model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "# #Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From e:\\UDemy Final\\Deep Learning NLP\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\UDemy Final\\Deep Learning NLP\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "644/644 [==============================] - 20s 24ms/step - loss: 6.8965 - accuracy: 0.0334 - val_loss: 6.7613 - val_accuracy: 0.0326\n",
      "Epoch 2/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 6.4683 - accuracy: 0.0386 - val_loss: 6.8442 - val_accuracy: 0.0350\n",
      "Epoch 3/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 6.3255 - accuracy: 0.0463 - val_loss: 6.8853 - val_accuracy: 0.0418\n",
      "Epoch 4/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 6.1834 - accuracy: 0.0541 - val_loss: 6.9457 - val_accuracy: 0.0455\n",
      "Epoch 5/50\n",
      "644/644 [==============================] - 14s 22ms/step - loss: 6.0507 - accuracy: 0.0552 - val_loss: 6.9807 - val_accuracy: 0.0459\n",
      "Epoch 6/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 5.9159 - accuracy: 0.0636 - val_loss: 7.0333 - val_accuracy: 0.0509\n",
      "Epoch 7/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 5.7781 - accuracy: 0.0734 - val_loss: 7.0298 - val_accuracy: 0.0573\n",
      "Epoch 8/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 5.6419 - accuracy: 0.0812 - val_loss: 7.0857 - val_accuracy: 0.0593\n",
      "Epoch 9/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 5.5222 - accuracy: 0.0886 - val_loss: 7.1543 - val_accuracy: 0.0589\n",
      "Epoch 10/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 5.4073 - accuracy: 0.0937 - val_loss: 7.2410 - val_accuracy: 0.0616\n",
      "Epoch 11/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 5.2956 - accuracy: 0.0996 - val_loss: 7.3234 - val_accuracy: 0.0643\n",
      "Epoch 12/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 5.1846 - accuracy: 0.1074 - val_loss: 7.4073 - val_accuracy: 0.0598\n",
      "Epoch 13/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 5.0773 - accuracy: 0.1096 - val_loss: 7.4926 - val_accuracy: 0.0661\n",
      "Epoch 14/50\n",
      "644/644 [==============================] - 16s 24ms/step - loss: 4.9680 - accuracy: 0.1151 - val_loss: 7.5843 - val_accuracy: 0.0635\n",
      "Epoch 15/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 4.8589 - accuracy: 0.1198 - val_loss: 7.6901 - val_accuracy: 0.0635\n",
      "Epoch 16/50\n",
      "644/644 [==============================] - 15s 24ms/step - loss: 4.7555 - accuracy: 0.1239 - val_loss: 7.8290 - val_accuracy: 0.0631\n",
      "Epoch 17/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 4.6535 - accuracy: 0.1293 - val_loss: 7.9189 - val_accuracy: 0.0668\n",
      "Epoch 18/50\n",
      "644/644 [==============================] - 14s 22ms/step - loss: 4.5460 - accuracy: 0.1335 - val_loss: 8.0612 - val_accuracy: 0.0614\n",
      "Epoch 19/50\n",
      "644/644 [==============================] - 14s 22ms/step - loss: 4.4424 - accuracy: 0.1389 - val_loss: 8.1920 - val_accuracy: 0.0639\n",
      "Epoch 20/50\n",
      "644/644 [==============================] - 16s 24ms/step - loss: 4.3456 - accuracy: 0.1473 - val_loss: 8.3489 - val_accuracy: 0.0637\n",
      "Epoch 21/50\n",
      "644/644 [==============================] - 16s 24ms/step - loss: 4.2514 - accuracy: 0.1528 - val_loss: 8.5151 - val_accuracy: 0.0624\n",
      "Epoch 22/50\n",
      "644/644 [==============================] - 16s 24ms/step - loss: 4.1674 - accuracy: 0.1631 - val_loss: 8.6254 - val_accuracy: 0.0628\n",
      "Epoch 23/50\n",
      "644/644 [==============================] - 15s 24ms/step - loss: 4.0828 - accuracy: 0.1707 - val_loss: 8.7845 - val_accuracy: 0.0608\n",
      "Epoch 24/50\n",
      "644/644 [==============================] - 14s 22ms/step - loss: 4.0061 - accuracy: 0.1833 - val_loss: 8.9135 - val_accuracy: 0.0595\n",
      "Epoch 25/50\n",
      "644/644 [==============================] - 14s 22ms/step - loss: 3.9280 - accuracy: 0.1956 - val_loss: 9.0477 - val_accuracy: 0.0587\n",
      "Epoch 26/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 3.8533 - accuracy: 0.2029 - val_loss: 9.1613 - val_accuracy: 0.0585\n",
      "Epoch 27/50\n",
      "644/644 [==============================] - 15s 24ms/step - loss: 3.7891 - accuracy: 0.2173 - val_loss: 9.2945 - val_accuracy: 0.0585\n",
      "Epoch 28/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 3.7213 - accuracy: 0.2293 - val_loss: 9.4233 - val_accuracy: 0.0626\n",
      "Epoch 29/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 3.6606 - accuracy: 0.2379 - val_loss: 9.5349 - val_accuracy: 0.0575\n",
      "Epoch 30/50\n",
      "644/644 [==============================] - 15s 24ms/step - loss: 3.5968 - accuracy: 0.2492 - val_loss: 9.6382 - val_accuracy: 0.0616\n",
      "Epoch 31/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 3.5456 - accuracy: 0.2548 - val_loss: 9.7361 - val_accuracy: 0.0589\n",
      "Epoch 32/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 3.4855 - accuracy: 0.2679 - val_loss: 9.8333 - val_accuracy: 0.0596\n",
      "Epoch 33/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 3.4324 - accuracy: 0.2765 - val_loss: 9.9608 - val_accuracy: 0.0595\n",
      "Epoch 34/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 3.3780 - accuracy: 0.2857 - val_loss: 10.0512 - val_accuracy: 0.0602\n",
      "Epoch 35/50\n",
      "644/644 [==============================] - 16s 24ms/step - loss: 3.3348 - accuracy: 0.2922 - val_loss: 10.1502 - val_accuracy: 0.0563\n",
      "Epoch 36/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 3.2829 - accuracy: 0.2999 - val_loss: 10.2287 - val_accuracy: 0.0596\n",
      "Epoch 37/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 3.2313 - accuracy: 0.3115 - val_loss: 10.3251 - val_accuracy: 0.0577\n",
      "Epoch 38/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.1836 - accuracy: 0.3179 - val_loss: 10.4313 - val_accuracy: 0.0571\n",
      "Epoch 39/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 3.1401 - accuracy: 0.3244 - val_loss: 10.5252 - val_accuracy: 0.0577\n",
      "Epoch 40/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 3.1024 - accuracy: 0.3316 - val_loss: 10.5853 - val_accuracy: 0.0581\n",
      "Epoch 41/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.0517 - accuracy: 0.3446 - val_loss: 10.6726 - val_accuracy: 0.0587\n",
      "Epoch 42/50\n",
      "644/644 [==============================] - 16s 26ms/step - loss: 3.0125 - accuracy: 0.3501 - val_loss: 10.7240 - val_accuracy: 0.0563\n",
      "Epoch 43/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 2.9737 - accuracy: 0.3532 - val_loss: 10.8403 - val_accuracy: 0.0573\n",
      "Epoch 44/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 2.9373 - accuracy: 0.3643 - val_loss: 10.8724 - val_accuracy: 0.0567\n",
      "Epoch 45/50\n",
      "644/644 [==============================] - 15s 24ms/step - loss: 2.8947 - accuracy: 0.3718 - val_loss: 11.0121 - val_accuracy: 0.0593\n",
      "Epoch 46/50\n",
      "644/644 [==============================] - 15s 24ms/step - loss: 2.8559 - accuracy: 0.3771 - val_loss: 11.0860 - val_accuracy: 0.0573\n",
      "Epoch 47/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 2.8224 - accuracy: 0.3863 - val_loss: 11.1461 - val_accuracy: 0.0558\n",
      "Epoch 48/50\n",
      "644/644 [==============================] - 15s 24ms/step - loss: 2.7804 - accuracy: 0.3915 - val_loss: 11.2065 - val_accuracy: 0.0587\n",
      "Epoch 49/50\n",
      "644/644 [==============================] - 14s 22ms/step - loss: 2.7528 - accuracy: 0.3964 - val_loss: 11.2703 - val_accuracy: 0.0579\n",
      "Epoch 50/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 2.7144 - accuracy: 0.4059 - val_loss: 11.3410 - val_accuracy: 0.0581\n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "history=model.fit(x_train,y_train,epochs=50,validation_data=(x_test,y_test),verbose=1,callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:To be or not to be\n",
      "Next Word PRediction:considered\n"
     ]
    }
   ],
   "source": [
    "input_text=\"To be or not to be\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word PRediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "model.save(\"next_word_lstm.h5\")\n",
    "## Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:  Barn. Last night of all,When yond same\n",
      "Next Word PRediction:cosin\n"
     ]
    }
   ],
   "source": [
    "input_text=\"  Barn. Last night of all,When yond same\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word PRediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
